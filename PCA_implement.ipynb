{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA manual implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implement Principal Component Analysis (or dimensionality reduction) for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \n",
    "    def __init__(self, num_components):\n",
    "        self.num_components = num_components\n",
    "        self.components     = None\n",
    "        self.mean           = None\n",
    "        self.variance_share = None\n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Find principal components\n",
    "        \"\"\"\n",
    "        \n",
    "        # data centering\n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        X        -= self.mean\n",
    "        \n",
    "        # calculate eigenvalues & vectors\n",
    "        cov_matrix      = np.cov(X.T)\n",
    "        values, vectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # sort eigenvalues & vectors \n",
    "        sort_idx = np.argsort(values)[::-1]\n",
    "        values   = values[sort_idx]\n",
    "        vectors  = vectors[:, sort_idx]\n",
    "        \n",
    "        # store principal components & variance\n",
    "        self.components = vectors[:self.num_components]\n",
    "        self.variance_share = np.sum(values[:self.num_components]) / np.sum(values)\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data\n",
    "        \"\"\"\n",
    "        \n",
    "        # data centering\n",
    "        X -= self.mean\n",
    "        \n",
    "        # decomposition\n",
    "        return np.dot(X, self.components.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Unnamed: 0', 'Station', 'Checks', 'AQI_bucket_calculated',\n",
       "       'AQI_bucket_calculated_shifted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset     \n",
    "df = pd.read_csv( \"../ML-Project-CS361/cleaned_shifted_data.csv\" ) \n",
    "drop_cols = [0,1,2,12,14,16]\n",
    "drop_cols = df.columns[drop_cols]\n",
    "drop_cols # Dropping unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns and make a new dataframe df1\n",
    "df.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174762, 11)\n"
     ]
    }
   ],
   "source": [
    "# The dataset has a size of 1,74,762 records, 10 features, 1 target variable\n",
    "print(df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174762, 10)\n",
      "(174762,)\n"
     ]
    }
   ],
   "source": [
    "# Separating the features and labels/target variables\n",
    "X = df.drop('AQI_calculated_shifted',axis = 1)  # feature set\n",
    "Y = df['AQI_calculated_shifted'] # target variable\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2284613 , -0.27947897, -0.40159894, ..., -0.46348286,\n",
       "        -0.37032615, -0.70256623],\n",
       "       [-0.2284613 , -0.27947897, -0.38000926, ..., -0.44749643,\n",
       "        -0.38256664, -0.69301701],\n",
       "       [-0.23480768, -0.2801947 , -0.37377224, ..., -0.44749643,\n",
       "        -0.39087269, -0.69301701],\n",
       "       ...,\n",
       "       [ 0.18906354,  0.02470543, -0.16123382, ...,  0.06406919,\n",
       "         1.86356376,  1.01629424],\n",
       "       [ 0.22246553,  0.02470543, -0.16123382, ...,  0.11202847,\n",
       "         0.88869596,  0.93035122],\n",
       "       [ 0.17236255,  1.06251102, -0.16123382, ...,  0.15998775,\n",
       "         0.53459599,  0.82530974]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the Data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15017421,  1.03560126, -1.25877373, ..., -0.14352518,\n",
       "        -1.06010037, -0.4568703 ],\n",
       "       [ 0.15744473,  1.08573516, -1.3682673 , ..., -0.23954184,\n",
       "        -1.25922592, -0.63391133],\n",
       "       [ 0.13229186,  1.0522327 , -1.28195604, ..., -0.17073515,\n",
       "        -1.11831908, -0.55303796],\n",
       "       ...,\n",
       "       [ 1.15463999, -1.12783814,  0.85077822, ..., -0.31840259,\n",
       "        -0.2475441 ,  0.22921917],\n",
       "       [ 0.70140958, -0.83965458,  0.54882366, ..., -0.693188  ,\n",
       "         0.60013108,  0.37158456],\n",
       "       [ 0.12059157, -0.96928581,  1.00131221, ..., -0.10002021,\n",
       "         0.38198446,  0.50255283]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carry out PCA to reduce the number of features from 10 to 7 components\n",
    "pca = PCA(num_components = 7)  # initialize PCA object\n",
    "pca.fit(X) # fit PCA on old data\n",
    "X_pca = pca.transform(X) # transform datasets\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174762, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.9473\n"
     ]
    }
   ],
   "source": [
    "# check explained variance\n",
    "print(f\"Explained variance: {pca.variance_share:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
